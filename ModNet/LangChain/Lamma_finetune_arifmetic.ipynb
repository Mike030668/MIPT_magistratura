{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1H9bx7EZTrj6A3xGl0aJTUsr6t55-GJ7d",
      "authorship_tag": "ABX9TyOdZMe8eamqUVKBWeD8ard3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mike030668/MIPT_magistratura/blob/main/ModNet/LangChain/Lamma_finetune_arifmetic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/liutiedong/goat.git -q"
      ],
      "metadata": {
        "id": "CkT4HtZlleiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/goat/requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeQP1TEOlvs9",
        "outputId": "5937a11e-9dbf-4497-afd6-290d05b56bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.6/381.6 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/Othercomputers/My_comp/Документы/МФТИ_Наука_о_данных/Современные_модели_DL/DZ_LangChain'\n",
        "model_path = path + '/model'\n",
        "data_path = path + '/dataset.json'"
      ],
      "metadata": {
        "id": "oUQUmsVqrS8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3KeCKazCJ29",
        "outputId": "fd392899-5c62-4395-8a0a-f9ad681fa998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.35.0-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.35.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "report - https://wandb.ai/mike-puzitskiy/Goat-7B/reports/Untitled-Report--Vmlldzo2MDE0MTk3"
      ],
      "metadata": {
        "id": "itvbFQSFe3NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/goat/finetune.py \\\n",
        "    --base_model 'baffo32/decapoda-research-llama-7B-hf' \\\n",
        "    --data_path {data_path} \\\n",
        "    --output_dir {model_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEFjBj3l84O",
        "outputId": "d5211f87-632c-4449-9718-ef294dcebb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-zwbbqsqxr3lz --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Training Alpaca-LoRA model with params:\n",
            "base_model: baffo32/decapoda-research-llama-7B-hf\n",
            "data_path: /content/drive/Othercomputers/My_comp/Документы/МФТИ_Наука_о_данных/Современные_модели_DL/DZ_LangChain/dataset.json\n",
            "output_dir: /content/drive/Othercomputers/My_comp/Документы/МФТИ_Наука_о_данных/Современные_модели_DL/DZ_LangChain/model\n",
            "batch_size: 128\n",
            "micro_batch_size: 16\n",
            "num_epochs: 1\n",
            "learning_rate: 0.0003\n",
            "cutoff_len: 512\n",
            "val_set_size: 0\n",
            "lora_r: 64\n",
            "lora_alpha: 64\n",
            "lora_dropout: 0.05\n",
            "lora_target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n",
            "train_on_inputs: False\n",
            "group_by_length: False\n",
            "wandb_project: Goat-7B\n",
            "wandb_run_name: \n",
            "wandb_watch: \n",
            "wandb_log_model: \n",
            "resume_from_checkpoint: False\n",
            "\n",
            "Loading checkpoint shards: 100% 33/33 [01:09<00:00,  2.11s/it]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "trainable params: 67108864 || all params: 6805524480 || trainable%: 0.986093932910223\n",
            "Map: 100% 308000/308000 [03:48<00:00, 1350.68 examples/s]\n",
            "2023-11-18 15:35:37.416317: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-18 15:35:37.416381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-18 15:35:37.416438: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-18 15:35:39.822371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20231118_154251-4wqcpc5h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-lion-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mike-puzitskiy/Goat-7B\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mike-puzitskiy/Goat-7B/runs/4wqcpc5h\u001b[0m\n",
            "  0% 0/2406 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "  0% 3/2406 [01:35<20:56:07, 31.36s/it]Lamma_arifmetic\n",
            "{'loss': 0.6527, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.0}\n",
            "{'loss': 0.4224, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2851, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2141, 'learning_rate': 0.00011999999999999999, 'epoch': 0.02}\n",
            "{'loss': 0.1547, 'learning_rate': 0.00015, 'epoch': 0.02}\n",
            "  2% 50/2406 [25:40<20:03:21, 30.65s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.1135, 'learning_rate': 0.00017999999999999998, 'epoch': 0.02}\n",
            "{'loss': 0.0819, 'learning_rate': 0.00020999999999999998, 'epoch': 0.03}\n",
            "{'loss': 0.0642, 'learning_rate': 0.00023999999999999998, 'epoch': 0.03}\n",
            "{'loss': 0.0511, 'learning_rate': 0.00027, 'epoch': 0.04}\n",
            "{'loss': 0.0401, 'learning_rate': 0.0003, 'epoch': 0.04}\n",
            "  4% 100/2406 [51:19<19:38:28, 30.66s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0465, 'learning_rate': 0.0002986990459670425, 'epoch': 0.05}\n",
            "{'loss': 0.0409, 'learning_rate': 0.00029739809193408495, 'epoch': 0.05}\n",
            "{'loss': 0.0371, 'learning_rate': 0.00029609713790112747, 'epoch': 0.05}\n",
            "{'loss': 0.0285, 'learning_rate': 0.00029479618386817, 'epoch': 0.06}\n",
            "{'loss': 0.0279, 'learning_rate': 0.00029349522983521245, 'epoch': 0.06}\n",
            "  6% 150/2406 [1:16:57<19:06:45, 30.50s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0277, 'learning_rate': 0.00029219427580225496, 'epoch': 0.07}\n",
            "{'loss': 0.0269, 'learning_rate': 0.0002908933217692975, 'epoch': 0.07}\n",
            "{'loss': 0.0254, 'learning_rate': 0.00028959236773633994, 'epoch': 0.07}\n",
            "{'loss': 0.0241, 'learning_rate': 0.00028829141370338246, 'epoch': 0.08}\n",
            "{'loss': 0.0236, 'learning_rate': 0.000286990459670425, 'epoch': 0.08}\n",
            "  8% 200/2406 [1:42:26<18:45:44, 30.62s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.039, 'learning_rate': 0.00028568950563746744, 'epoch': 0.09}\n",
            "{'loss': 0.0268, 'learning_rate': 0.00028438855160450995, 'epoch': 0.09}\n",
            "{'loss': 0.0246, 'learning_rate': 0.00028308759757155247, 'epoch': 0.1}\n",
            "{'loss': 0.0193, 'learning_rate': 0.00028178664353859493, 'epoch': 0.1}\n",
            "{'loss': 0.0186, 'learning_rate': 0.00028048568950563745, 'epoch': 0.1}\n",
            " 10% 250/2406 [2:08:00<18:05:29, 30.21s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0159, 'learning_rate': 0.00027918473547267997, 'epoch': 0.11}\n",
            "{'loss': 0.0155, 'learning_rate': 0.00027788378143972243, 'epoch': 0.11}\n",
            "{'loss': 0.0176, 'learning_rate': 0.00027658282740676494, 'epoch': 0.12}\n",
            "{'loss': 0.0152, 'learning_rate': 0.00027528187337380746, 'epoch': 0.12}\n",
            "{'loss': 0.0154, 'learning_rate': 0.0002739809193408499, 'epoch': 0.12}\n",
            " 12% 300/2406 [2:33:27<17:48:39, 30.45s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.026, 'learning_rate': 0.00027267996530789244, 'epoch': 0.13}\n",
            "{'loss': 0.0262, 'learning_rate': 0.00027137901127493496, 'epoch': 0.13}\n",
            "{'loss': 0.0177, 'learning_rate': 0.0002700780572419774, 'epoch': 0.14}\n",
            "{'loss': 0.0157, 'learning_rate': 0.00026877710320901994, 'epoch': 0.14}\n",
            "{'loss': 0.0146, 'learning_rate': 0.00026747614917606245, 'epoch': 0.15}\n",
            " 15% 350/2406 [2:59:03<17:27:01, 30.56s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0131, 'learning_rate': 0.0002661751951431049, 'epoch': 0.15}\n",
            "{'loss': 0.0133, 'learning_rate': 0.00026487424111014743, 'epoch': 0.15}\n",
            "{'loss': 0.0141, 'learning_rate': 0.00026357328707718995, 'epoch': 0.16}\n",
            "{'loss': 0.0146, 'learning_rate': 0.0002622723330442324, 'epoch': 0.16}\n",
            "{'loss': 0.0147, 'learning_rate': 0.0002609713790112749, 'epoch': 0.17}\n",
            " 17% 400/2406 [3:24:41<17:12:56, 30.90s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.018, 'learning_rate': 0.00025967042497831744, 'epoch': 0.17}\n",
            "{'loss': 0.0252, 'learning_rate': 0.0002583694709453599, 'epoch': 0.17}\n",
            "{'loss': 0.0173, 'learning_rate': 0.0002570685169124024, 'epoch': 0.18}\n",
            "{'loss': 0.0189, 'learning_rate': 0.0002557675628794449, 'epoch': 0.18}\n",
            "{'loss': 0.0149, 'learning_rate': 0.0002544666088464874, 'epoch': 0.19}\n",
            " 19% 450/2406 [3:50:08<16:20:53, 30.09s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0142, 'learning_rate': 0.0002531656548135299, 'epoch': 0.19}\n",
            "{'loss': 0.0121, 'learning_rate': 0.0002518647007805724, 'epoch': 0.2}\n",
            "{'loss': 0.0113, 'learning_rate': 0.0002505637467476149, 'epoch': 0.2}\n",
            "{'loss': 0.0108, 'learning_rate': 0.0002492627927146574, 'epoch': 0.2}\n",
            "{'loss': 0.0109, 'learning_rate': 0.0002479618386816999, 'epoch': 0.21}\n",
            " 21% 500/2406 [4:15:34<15:53:15, 30.01s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0106, 'learning_rate': 0.0002466608846487424, 'epoch': 0.21}\n",
            "{'loss': 0.0113, 'learning_rate': 0.0002453599306157849, 'epoch': 0.22}\n",
            "{'loss': 0.0097, 'learning_rate': 0.00024405897658282737, 'epoch': 0.22}\n",
            "{'loss': 0.0095, 'learning_rate': 0.00024275802254986989, 'epoch': 0.22}\n",
            "{'loss': 0.0097, 'learning_rate': 0.0002414570685169124, 'epoch': 0.23}\n",
            " 23% 550/2406 [4:41:04<15:40:18, 30.40s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0103, 'learning_rate': 0.00024015611448395486, 'epoch': 0.23}\n",
            "{'loss': 0.0097, 'learning_rate': 0.00023885516045099738, 'epoch': 0.24}\n",
            "{'loss': 0.0099, 'learning_rate': 0.00023755420641803987, 'epoch': 0.24}\n",
            "{'loss': 0.0093, 'learning_rate': 0.00023625325238508236, 'epoch': 0.25}\n",
            "{'loss': 0.0092, 'learning_rate': 0.00023495229835212488, 'epoch': 0.25}\n",
            " 25% 600/2406 [5:06:28<15:14:49, 30.39s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0106, 'learning_rate': 0.00023365134431916737, 'epoch': 0.25}\n",
            "{'loss': 0.0114, 'learning_rate': 0.00023235039028620985, 'epoch': 0.26}\n",
            "{'loss': 0.0112, 'learning_rate': 0.00023104943625325237, 'epoch': 0.26}\n",
            "{'loss': 0.0116, 'learning_rate': 0.00022974848222029486, 'epoch': 0.27}\n",
            "{'loss': 0.0174, 'learning_rate': 0.00022844752818733735, 'epoch': 0.27}\n",
            " 27% 650/2406 [5:31:54<14:37:48, 29.99s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.024, 'learning_rate': 0.00022714657415437987, 'epoch': 0.27}\n",
            "{'loss': 0.0155, 'learning_rate': 0.00022584562012142236, 'epoch': 0.28}\n",
            "{'loss': 0.0127, 'learning_rate': 0.00022454466608846484, 'epoch': 0.28}\n",
            "{'loss': 0.0109, 'learning_rate': 0.00022324371205550736, 'epoch': 0.29}\n",
            "{'loss': 0.0101, 'learning_rate': 0.00022194275802254985, 'epoch': 0.29}\n",
            " 29% 700/2406 [5:57:31<14:28:28, 30.54s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0088, 'learning_rate': 0.00022064180398959234, 'epoch': 0.3}\n",
            "{'loss': 0.0091, 'learning_rate': 0.00021934084995663486, 'epoch': 0.3}\n",
            "{'loss': 0.0098, 'learning_rate': 0.00021803989592367735, 'epoch': 0.3}\n",
            "{'loss': 0.01, 'learning_rate': 0.00021673894189071984, 'epoch': 0.31}\n",
            "{'loss': 0.0097, 'learning_rate': 0.00021543798785776235, 'epoch': 0.31}\n",
            " 31% 750/2406 [6:22:57<14:02:49, 30.54s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0089, 'learning_rate': 0.00021413703382480484, 'epoch': 0.32}\n",
            "{'loss': 0.01, 'learning_rate': 0.00021283607979184733, 'epoch': 0.32}\n",
            "{'loss': 0.0099, 'learning_rate': 0.00021153512575888985, 'epoch': 0.32}\n",
            "{'loss': 0.009, 'learning_rate': 0.00021023417172593234, 'epoch': 0.33}\n",
            "{'loss': 0.0102, 'learning_rate': 0.00020893321769297483, 'epoch': 0.33}\n",
            " 33% 800/2406 [6:48:26<13:33:11, 30.38s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0113, 'learning_rate': 0.00020763226366001731, 'epoch': 0.34}\n",
            "{'loss': 0.009, 'learning_rate': 0.00020633130962705983, 'epoch': 0.34}\n",
            "{'loss': 0.0082, 'learning_rate': 0.00020503035559410232, 'epoch': 0.34}\n",
            "{'loss': 0.0088, 'learning_rate': 0.0002037294015611448, 'epoch': 0.35}\n",
            "{'loss': 0.0084, 'learning_rate': 0.00020242844752818733, 'epoch': 0.35}\n",
            " 35% 850/2406 [7:13:47<12:58:43, 30.03s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0078, 'learning_rate': 0.00020112749349522982, 'epoch': 0.36}\n",
            "{'loss': 0.0069, 'learning_rate': 0.0001998265394622723, 'epoch': 0.36}\n",
            "{'loss': 0.0085, 'learning_rate': 0.00019852558542931482, 'epoch': 0.37}\n",
            "{'loss': 0.0075, 'learning_rate': 0.0001972246313963573, 'epoch': 0.37}\n",
            "{'loss': 0.0068, 'learning_rate': 0.0001959236773633998, 'epoch': 0.37}\n",
            " 37% 900/2406 [7:39:05<12:36:21, 30.13s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0061, 'learning_rate': 0.00019462272333044232, 'epoch': 0.38}\n",
            "{'loss': 0.0073, 'learning_rate': 0.0001933217692974848, 'epoch': 0.38}\n",
            "{'loss': 0.0076, 'learning_rate': 0.0001920208152645273, 'epoch': 0.39}\n",
            "{'loss': 0.0071, 'learning_rate': 0.0001907198612315698, 'epoch': 0.39}\n",
            "{'loss': 0.0064, 'learning_rate': 0.0001894189071986123, 'epoch': 0.39}\n",
            " 39% 950/2406 [8:04:25<12:13:31, 30.23s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0081, 'learning_rate': 0.0001881179531656548, 'epoch': 0.4}\n",
            "{'loss': 0.0087, 'learning_rate': 0.0001868169991326973, 'epoch': 0.4}\n",
            "{'loss': 0.0078, 'learning_rate': 0.0001855160450997398, 'epoch': 0.41}\n",
            "{'loss': 0.0084, 'learning_rate': 0.00018421509106678229, 'epoch': 0.41}\n",
            "{'loss': 0.0084, 'learning_rate': 0.0001829141370338248, 'epoch': 0.42}\n",
            " 42% 1000/2406 [8:29:56<11:40:53, 29.91s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0072, 'learning_rate': 0.0001816131830008673, 'epoch': 0.42}\n",
            "{'loss': 0.0072, 'learning_rate': 0.00018031222896790978, 'epoch': 0.42}\n",
            "{'loss': 0.0071, 'learning_rate': 0.0001790112749349523, 'epoch': 0.43}\n",
            "{'loss': 0.0075, 'learning_rate': 0.0001777103209019948, 'epoch': 0.43}\n",
            "{'loss': 0.0066, 'learning_rate': 0.00017640936686903728, 'epoch': 0.44}\n",
            " 44% 1050/2406 [8:55:27<11:31:03, 30.58s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0057, 'learning_rate': 0.0001751084128360798, 'epoch': 0.44}\n",
            "{'loss': 0.0068, 'learning_rate': 0.00017380745880312226, 'epoch': 0.44}\n",
            "{'loss': 0.0079, 'learning_rate': 0.00017250650477016477, 'epoch': 0.45}\n",
            "{'loss': 0.0068, 'learning_rate': 0.0001712055507372073, 'epoch': 0.45}\n",
            "{'loss': 0.0071, 'learning_rate': 0.00016990459670424975, 'epoch': 0.46}\n",
            " 46% 1100/2406 [9:20:48<11:04:42, 30.54s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0076, 'learning_rate': 0.00016860364267129227, 'epoch': 0.46}\n",
            "{'loss': 0.008, 'learning_rate': 0.00016730268863833476, 'epoch': 0.47}\n",
            "{'loss': 0.007, 'learning_rate': 0.00016600173460537725, 'epoch': 0.47}\n",
            "{'loss': 0.0072, 'learning_rate': 0.00016470078057241976, 'epoch': 0.47}\n",
            "{'loss': 0.0105, 'learning_rate': 0.00016339982653946225, 'epoch': 0.48}\n",
            " 48% 1150/2406 [9:46:15<10:45:33, 30.84s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0079, 'learning_rate': 0.00016209887250650474, 'epoch': 0.48}\n",
            "{'loss': 0.0064, 'learning_rate': 0.00016079791847354726, 'epoch': 0.49}\n",
            "{'loss': 0.0063, 'learning_rate': 0.00015949696444058975, 'epoch': 0.49}\n",
            "{'loss': 0.0086, 'learning_rate': 0.00015819601040763224, 'epoch': 0.49}\n",
            "{'loss': 0.0077, 'learning_rate': 0.00015689505637467475, 'epoch': 0.5}\n",
            " 50% 1200/2406 [10:11:38<10:07:16, 30.21s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0073, 'learning_rate': 0.00015559410234171724, 'epoch': 0.5}\n",
            "{'loss': 0.0067, 'learning_rate': 0.00015429314830875973, 'epoch': 0.51}\n",
            "{'loss': 0.0066, 'learning_rate': 0.00015299219427580225, 'epoch': 0.51}\n",
            "{'loss': 0.0062, 'learning_rate': 0.00015169124024284474, 'epoch': 0.52}\n",
            "{'loss': 0.0047, 'learning_rate': 0.00015039028620988723, 'epoch': 0.52}\n",
            " 52% 1250/2406 [10:36:52<9:42:39, 30.24s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0064, 'learning_rate': 0.00014908933217692974, 'epoch': 0.52}\n",
            "{'loss': 0.0073, 'learning_rate': 0.00014778837814397223, 'epoch': 0.53}\n",
            "{'loss': 0.0071, 'learning_rate': 0.00014648742411101472, 'epoch': 0.53}\n",
            "{'loss': 0.0067, 'learning_rate': 0.0001451864700780572, 'epoch': 0.54}\n",
            "{'loss': 0.0057, 'learning_rate': 0.00014388551604509973, 'epoch': 0.54}\n",
            " 54% 1300/2406 [11:02:16<9:19:55, 30.38s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0062, 'learning_rate': 0.00014258456201214222, 'epoch': 0.54}\n",
            "{'loss': 0.007, 'learning_rate': 0.0001412836079791847, 'epoch': 0.55}\n",
            "{'loss': 0.0066, 'learning_rate': 0.00013998265394622722, 'epoch': 0.55}\n",
            "{'loss': 0.0058, 'learning_rate': 0.0001386816999132697, 'epoch': 0.56}\n",
            "{'loss': 0.005, 'learning_rate': 0.0001373807458803122, 'epoch': 0.56}\n",
            " 56% 1350/2406 [11:27:31<8:48:28, 30.03s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0055, 'learning_rate': 0.00013607979184735472, 'epoch': 0.57}\n",
            "{'loss': 0.006, 'learning_rate': 0.0001347788378143972, 'epoch': 0.57}\n",
            "{'loss': 0.0066, 'learning_rate': 0.0001334778837814397, 'epoch': 0.57}\n",
            "{'loss': 0.0047, 'learning_rate': 0.0001321769297484822, 'epoch': 0.58}\n",
            "{'loss': 0.0048, 'learning_rate': 0.0001308759757155247, 'epoch': 0.58}\n",
            " 58% 1400/2406 [11:52:50<8:29:28, 30.39s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0047, 'learning_rate': 0.0001295750216825672, 'epoch': 0.59}\n",
            "{'loss': 0.0053, 'learning_rate': 0.0001282740676496097, 'epoch': 0.59}\n",
            "{'loss': 0.0055, 'learning_rate': 0.0001269731136166522, 'epoch': 0.59}\n",
            "{'loss': 0.0064, 'learning_rate': 0.0001256721595836947, 'epoch': 0.6}\n",
            "{'loss': 0.0072, 'learning_rate': 0.0001243712055507372, 'epoch': 0.6}\n",
            " 60% 1450/2406 [12:18:13<8:02:52, 30.31s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0074, 'learning_rate': 0.0001230702515177797, 'epoch': 0.61}\n",
            "{'loss': 0.0062, 'learning_rate': 0.00012176929748482218, 'epoch': 0.61}\n",
            "{'loss': 0.0057, 'learning_rate': 0.0001204683434518647, 'epoch': 0.62}\n",
            "{'loss': 0.0054, 'learning_rate': 0.00011916738941890719, 'epoch': 0.62}\n",
            "{'loss': 0.0047, 'learning_rate': 0.00011786643538594968, 'epoch': 0.62}\n",
            " 62% 1500/2406 [12:43:40<7:41:56, 30.59s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0053, 'learning_rate': 0.0001165654813529922, 'epoch': 0.63}\n",
            "{'loss': 0.0045, 'learning_rate': 0.00011526452732003468, 'epoch': 0.63}\n",
            "{'loss': 0.0047, 'learning_rate': 0.00011396357328707717, 'epoch': 0.64}\n",
            "{'loss': 0.0048, 'learning_rate': 0.00011266261925411969, 'epoch': 0.64}\n",
            "{'loss': 0.0049, 'learning_rate': 0.00011136166522116218, 'epoch': 0.64}\n",
            " 64% 1550/2406 [13:09:02<7:06:34, 29.90s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0052, 'learning_rate': 0.00011006071118820467, 'epoch': 0.65}\n",
            "{'loss': 0.0061, 'learning_rate': 0.00010875975715524718, 'epoch': 0.65}\n",
            "{'loss': 0.0057, 'learning_rate': 0.00010745880312228967, 'epoch': 0.66}\n",
            "{'loss': 0.0049, 'learning_rate': 0.00010615784908933216, 'epoch': 0.66}\n",
            "{'loss': 0.0049, 'learning_rate': 0.00010485689505637468, 'epoch': 0.66}\n",
            " 67% 1600/2406 [13:34:18<6:44:59, 30.15s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.005, 'learning_rate': 0.00010355594102341717, 'epoch': 0.67}\n",
            "{'loss': 0.0046, 'learning_rate': 0.00010225498699045966, 'epoch': 0.67}\n",
            "{'loss': 0.0047, 'learning_rate': 0.00010095403295750215, 'epoch': 0.68}\n",
            "{'loss': 0.0046, 'learning_rate': 9.965307892454466e-05, 'epoch': 0.68}\n",
            "{'loss': 0.0041, 'learning_rate': 9.835212489158715e-05, 'epoch': 0.69}\n",
            " 69% 1650/2406 [13:59:34<6:23:57, 30.47s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.004, 'learning_rate': 9.705117085862964e-05, 'epoch': 0.69}\n",
            "{'loss': 0.0037, 'learning_rate': 9.575021682567216e-05, 'epoch': 0.69}\n",
            "{'loss': 0.0048, 'learning_rate': 9.444926279271465e-05, 'epoch': 0.7}\n",
            "{'loss': 0.0041, 'learning_rate': 9.314830875975714e-05, 'epoch': 0.7}\n",
            "{'loss': 0.0049, 'learning_rate': 9.184735472679965e-05, 'epoch': 0.71}\n",
            " 71% 1700/2406 [14:24:55<6:00:39, 30.65s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0042, 'learning_rate': 9.054640069384214e-05, 'epoch': 0.71}\n",
            "{'loss': 0.0042, 'learning_rate': 8.924544666088463e-05, 'epoch': 0.71}\n",
            "{'loss': 0.0042, 'learning_rate': 8.794449262792715e-05, 'epoch': 0.72}\n",
            "{'loss': 0.0036, 'learning_rate': 8.664353859496964e-05, 'epoch': 0.72}\n",
            "{'loss': 0.005, 'learning_rate': 8.534258456201213e-05, 'epoch': 0.73}\n",
            " 73% 1750/2406 [14:50:15<5:28:27, 30.04s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0049, 'learning_rate': 8.404163052905463e-05, 'epoch': 0.73}\n",
            "{'loss': 0.0041, 'learning_rate': 8.274067649609713e-05, 'epoch': 0.74}\n",
            "{'loss': 0.0045, 'learning_rate': 8.143972246313962e-05, 'epoch': 0.74}\n",
            "{'loss': 0.0041, 'learning_rate': 8.013876843018213e-05, 'epoch': 0.74}\n",
            "{'loss': 0.0038, 'learning_rate': 7.883781439722463e-05, 'epoch': 0.75}\n",
            " 75% 1800/2406 [15:15:35<5:05:28, 30.25s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0049, 'learning_rate': 7.753686036426712e-05, 'epoch': 0.75}\n",
            "{'loss': 0.004, 'learning_rate': 7.623590633130962e-05, 'epoch': 0.76}\n",
            "{'loss': 0.0046, 'learning_rate': 7.493495229835212e-05, 'epoch': 0.76}\n",
            "{'loss': 0.0035, 'learning_rate': 7.363399826539461e-05, 'epoch': 0.76}\n",
            "{'loss': 0.0037, 'learning_rate': 7.233304423243712e-05, 'epoch': 0.77}\n",
            " 77% 1850/2406 [15:40:52<4:38:30, 30.06s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0041, 'learning_rate': 7.103209019947962e-05, 'epoch': 0.77}\n",
            "{'loss': 0.0042, 'learning_rate': 6.973113616652211e-05, 'epoch': 0.78}\n",
            "{'loss': 0.0046, 'learning_rate': 6.843018213356461e-05, 'epoch': 0.78}\n",
            "{'loss': 0.0038, 'learning_rate': 6.71292281006071e-05, 'epoch': 0.79}\n",
            "{'loss': 0.004, 'learning_rate': 6.58282740676496e-05, 'epoch': 0.79}\n",
            " 79% 1900/2406 [16:06:10<4:15:52, 30.34s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0036, 'learning_rate': 6.452732003469211e-05, 'epoch': 0.79}\n",
            "{'loss': 0.0036, 'learning_rate': 6.32263660017346e-05, 'epoch': 0.8}\n",
            "{'loss': 0.0034, 'learning_rate': 6.19254119687771e-05, 'epoch': 0.8}\n",
            "{'loss': 0.0033, 'learning_rate': 6.0624457935819596e-05, 'epoch': 0.81}\n",
            "{'loss': 0.0033, 'learning_rate': 5.93235039028621e-05, 'epoch': 0.81}\n",
            " 81% 1950/2406 [16:31:23<3:46:42, 29.83s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0034, 'learning_rate': 5.802254986990459e-05, 'epoch': 0.81}\n",
            "{'loss': 0.0039, 'learning_rate': 5.672159583694709e-05, 'epoch': 0.82}\n",
            "{'loss': 0.0033, 'learning_rate': 5.5420641803989594e-05, 'epoch': 0.82}\n",
            "{'loss': 0.003, 'learning_rate': 5.411968777103208e-05, 'epoch': 0.83}\n",
            "{'loss': 0.0033, 'learning_rate': 5.2818733738074586e-05, 'epoch': 0.83}\n",
            " 83% 2000/2406 [16:56:35<3:25:16, 30.34s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0034, 'learning_rate': 5.151777970511708e-05, 'epoch': 0.84}\n",
            "{'loss': 0.0031, 'learning_rate': 5.021682567215958e-05, 'epoch': 0.84}\n",
            "{'loss': 0.0036, 'learning_rate': 4.891587163920208e-05, 'epoch': 0.84}\n",
            "{'loss': 0.0041, 'learning_rate': 4.761491760624458e-05, 'epoch': 0.85}\n",
            "{'loss': 0.0034, 'learning_rate': 4.6313963573287074e-05, 'epoch': 0.85}\n",
            " 85% 2050/2406 [17:21:50<2:58:52, 30.15s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.003, 'learning_rate': 4.501300954032957e-05, 'epoch': 0.86}\n",
            "{'loss': 0.0026, 'learning_rate': 4.3712055507372066e-05, 'epoch': 0.86}\n",
            "{'loss': 0.0033, 'learning_rate': 4.241110147441457e-05, 'epoch': 0.86}\n",
            "{'loss': 0.0029, 'learning_rate': 4.1110147441457065e-05, 'epoch': 0.87}\n",
            "{'loss': 0.0036, 'learning_rate': 3.980919340849956e-05, 'epoch': 0.87}\n",
            " 87% 2100/2406 [17:47:07<2:33:16, 30.05s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0026, 'learning_rate': 3.850823937554206e-05, 'epoch': 0.88}\n",
            "{'loss': 0.0031, 'learning_rate': 3.720728534258456e-05, 'epoch': 0.88}\n",
            "{'loss': 0.0028, 'learning_rate': 3.5906331309627056e-05, 'epoch': 0.89}\n",
            "{'loss': 0.0031, 'learning_rate': 3.460537727666955e-05, 'epoch': 0.89}\n",
            "{'loss': 0.0035, 'learning_rate': 3.330442324371205e-05, 'epoch': 0.89}\n",
            " 89% 2150/2406 [18:12:28<2:09:46, 30.42s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0026, 'learning_rate': 3.200346921075455e-05, 'epoch': 0.9}\n",
            "{'loss': 0.0029, 'learning_rate': 3.070251517779705e-05, 'epoch': 0.9}\n",
            "{'loss': 0.0031, 'learning_rate': 2.9401561144839547e-05, 'epoch': 0.91}\n",
            "{'loss': 0.0031, 'learning_rate': 2.8100607111882043e-05, 'epoch': 0.91}\n",
            "{'loss': 0.0027, 'learning_rate': 2.6799653078924543e-05, 'epoch': 0.91}\n",
            " 91% 2200/2406 [18:37:44<1:42:06, 29.74s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0027, 'learning_rate': 2.549869904596704e-05, 'epoch': 0.92}\n",
            "{'loss': 0.0025, 'learning_rate': 2.4197745013009535e-05, 'epoch': 0.92}\n",
            "{'loss': 0.0024, 'learning_rate': 2.2896790980052038e-05, 'epoch': 0.93}\n",
            "{'loss': 0.0027, 'learning_rate': 2.1595836947094534e-05, 'epoch': 0.93}\n",
            "{'loss': 0.0027, 'learning_rate': 2.029488291413703e-05, 'epoch': 0.94}\n",
            " 94% 2250/2406 [19:03:09<1:18:54, 30.35s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0027, 'learning_rate': 1.8993928881179533e-05, 'epoch': 0.94}\n",
            "{'loss': 0.0027, 'learning_rate': 1.769297484822203e-05, 'epoch': 0.94}\n",
            "{'loss': 0.0024, 'learning_rate': 1.6392020815264526e-05, 'epoch': 0.95}\n",
            "{'loss': 0.0024, 'learning_rate': 1.5091066782307025e-05, 'epoch': 0.95}\n",
            "{'loss': 0.0026, 'learning_rate': 1.3790112749349521e-05, 'epoch': 0.96}\n",
            " 96% 2300/2406 [19:28:23<52:36, 29.78s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0026, 'learning_rate': 1.2489158716392019e-05, 'epoch': 0.96}\n",
            "{'loss': 0.0024, 'learning_rate': 1.1188204683434519e-05, 'epoch': 0.96}\n",
            "{'loss': 0.0026, 'learning_rate': 9.887250650477015e-06, 'epoch': 0.97}\n",
            "{'loss': 0.0029, 'learning_rate': 8.586296617519513e-06, 'epoch': 0.97}\n",
            "{'loss': 0.0029, 'learning_rate': 7.285342584562012e-06, 'epoch': 0.98}\n",
            " 98% 2350/2406 [19:53:29<27:47, 29.78s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0028, 'learning_rate': 5.98438855160451e-06, 'epoch': 0.98}\n",
            "{'loss': 0.0024, 'learning_rate': 4.683434518647008e-06, 'epoch': 0.98}\n",
            "{'loss': 0.0027, 'learning_rate': 3.382480485689505e-06, 'epoch': 0.99}\n",
            "{'loss': 0.0023, 'learning_rate': 2.081526452732003e-06, 'epoch': 0.99}\n",
            "{'loss': 0.0025, 'learning_rate': 7.805724197745012e-07, 'epoch': 1.0}\n",
            "100% 2400/2406 [20:18:44<03:01, 30.18s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'train_runtime': 73738.7115, 'train_samples_per_second': 4.177, 'train_steps_per_second': 0.033, 'train_loss': 0.016903384222477946, 'epoch': 1.0}\n",
            "100% 2406/2406 [20:21:53<00:00, 30.47s/it]\n",
            "\n",
            " If there's a warning about missing keys above, please disregard :)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▂▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 2406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.0025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.0305833053873766e+18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.0169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 73738.7115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 4.177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdainty-lion-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mike-puzitskiy/Goat-7B/runs/4wqcpc5h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231118_154251-4wqcpc5h/logs\u001b[0m\n",
            "Exception in thread IntMsgThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 300, in check_internal_messages\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 766, in deliver_internal_messages\n",
            "    return self._deliver_internal_messages(internal_message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 490, in _deliver_internal_messages\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 437, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 268, in check_network_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 758, in deliver_network_status\n",
            "    return self._deliver_network_status(status)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 484, in _deliver_network_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 437, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    }
  ]
}