{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "14TPTrFHCglJUE5SV87zHwqhY7GBwlKwI",
      "authorship_tag": "ABX9TyOqCt/2gnEcqOMtjdkCzyZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mike030668/MIPT_magistratura/blob/main/NLP_generation%20/Project_retrivial_bot/Context_talk_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZOgueJ5oHhN"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Mike030668/MIPT_magistratura.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vGBwQ9n3td2"
      },
      "outputs": [],
      "source": [
        "import gdown  # библиотека по работе с файлами в том числе и с гугл_диска\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAIN_DIR =  \"/content/MIPT_magistratura/NLP_generation/Project_retrivial_bot\"\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "sys.path.append(MAIN_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f54759-d65e-473a-a73f-70bd9fb2b0cf"
      },
      "source": [
        "# Load database"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.utils import get_replies, load_weights, CrossEncoderBert\n",
        "\n",
        "path_df = MAIN_DIR + \"/data/talks_dataset.df\"\n",
        "talks_df = pd.read_pickle(path_df)\n",
        "\n",
        "all_replies = get_replies(talks_df)"
      ],
      "metadata": {
        "id": "ocEGSj5jVL_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "4JQrdYqb9WIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/file/d/1yj0VuOJmpwio0h9cLuBs50oG_YzVr86a/view?usp=sharing'\n",
        "weights =  '/model.pt'\n",
        "load_weights(url_weights = url, name_model = \"cross_encoder\", name_weights = weights, main_dir = MAIN_DIR)\n",
        "\n",
        "cross_model = torch.load(MAIN_DIR +\"/models/cross_encoder/model.pt\")\n",
        "cross_model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGW5sWLb6cVk",
        "outputId": "3d54750a-470e-44f5-d3ff-bacdf3f07e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yj0VuOJmpwio0h9cLuBs50oG_YzVr86a\n",
            "From (redirected): https://drive.google.com/uc?id=1yj0VuOJmpwio0h9cLuBs50oG_YzVr86a&confirm=t&uuid=eec11bbf-1f68-45dd-afb9-44c994e57ba2\n",
            "To: /content/MIPT_magistratura/NLP_generation/Project_retrivial_bot/models/cross_encoder/model.pt\n",
            "100%|██████████| 266M/266M [00:01<00:00, 143MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEncoderBert(\n",
              "  (bert_model): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вопрос ответ"
      ],
      "metadata": {
        "id": "BfY05DH4P_7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.talk_context import flush_memory, get_best_rand_reply"
      ],
      "metadata": {
        "id": "w1c6mAcpctMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the live?\"  #\n",
        "print(f\"Реплика: {question}\")\n",
        "\n",
        "best_answer, conext_memory,  best_score = get_best_rand_reply(\n",
        "    cross_model,\n",
        "    query = question,\n",
        "    context = \"\",\n",
        "    corpus = all_replies,\n",
        "    max_length=MAX_LENGTH,\n",
        "    device = DEVICE\n",
        "    )\n",
        "\n",
        "print(f\"Лучший ответ: {best_answer}\\nscore {best_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9560c751-a19e-4fb7-93b7-d94c88a87aa9",
        "id": "TP9zz-pwDrpV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Реплика: What is the live?\n",
            "Лучший ответ: But sometimes, after the Siamese twins are joined together, one of the twins dies before birth. The living baby is born with the dead baby still attached.  Sometimes, this dead twin is inside the living person, so even you could have a dead twin inside you and not even know it! \n",
            "score 0.9702920913696289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Иммитация беседы с контекстом"
      ],
      "metadata": {
        "id": "5q_j5_RAPzas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con = 0\n",
        "question = input('Реплика: ')\n",
        "while question != 'stop':\n",
        "  #print(f\"Реплика: {question}\")\n",
        "  if not con: conext_memory = \"\"\n",
        "  best_answer, conext_memory, best_score = get_best_rand_reply(\n",
        "                              cross_model,\n",
        "                              query = question,\n",
        "                              context = conext_memory,\n",
        "                              corpus = all_replies,\n",
        "                              max_length=MAX_LENGTH,\n",
        "                              device = DEVICE\n",
        "                              )\n",
        "  print(f\"Ответ: {best_answer}\\n\")\n",
        "  con+=1\n",
        "  question = input('Реплика: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HnibEs7fn9v",
        "outputId": "37b1b426-4443-43b9-dad2-5674eaa55cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Реплика: Hello\n",
            "Ответ: Little Bunny Foo-Foo hoppin' through the forestScoopin' up the field mice and boppin' 'em on the headDown came a white angel and she said\"Little Bunny Foo-Foo, I don't wanna see youScoopin' up the field mice and boppin' 'em on the head\"So now I'm gonna turn you into a worm, mbuh, mBunny Foo-Foo... Eh, beh-POOFLittle Wormy Foo-Foo crawlin' through the forestGettin scooped up by the field mice who mah-m then they bopped 'im on the head\n",
            "\n",
            "Реплика: How are you?\n",
            "Ответ: Stan, nunh tunh tunh tunh tunh tunh tunh tunh tunh tunh tunh tunh hunh. Tunh tunh tunh tunh tinh teenh?\n",
            "\n",
            "Реплика: What do think about rest on the beach?\n",
            "Ответ: Kyle! Kyle! Get the... Get the- Kyle! Kyle, dude, that was- that was in the balls! Dude, serious- seriously! That was in the balls! No hitting- No hitting in the balls!  Kyle!  Quit it, Kyle. Give it up, Kyle!  Okay... Okay! Okay okay!  You win! You win. I give up.  Hahaha! I had my fingers crossed. \n",
            "\n",
            "Реплика: Let's go together?\n",
            "Ответ: Timmih! Timmih, uh, Jimmih.  Hey Timmy, Timmih Tim-oh! Tim-Timmih!  Timmmih? Jimmih! Jimmih.  Jimmih Timmih Timmih? Timmih, Timmih! Jimmih... Jimmih! Jimmih! Timmih!  Timmih!\n",
            "\n",
            "Реплика: Why so\n",
            "Ответ: Hut-hut-hut-hut-hut--hut. hut. hut-hut-hut-hut-hut-hut-hut.\n",
            "\n",
            "Реплика: Are you ok?\n",
            "Ответ: He's here, Ohh my God  Omigod omigod omigod omigod omigod omigod omigod omigod omigod o-\n",
            "\n",
            "Реплика: ok. I m going alone!!!\n",
            "Ответ: Yummy yummy yummy yummy yummy yummy yummy yummy yummy yummy.\n",
            "\n",
            "Реплика: Bye\n",
            "Ответ: We have-uh repeatedly broken God's commandments-uh! We have lived our lives for ourselves-uh! Totally ignoring the Lord-uh!  If thy hand offend thee, cut it off!  It is much better for thee to enter into life maimed-\n",
            "\n",
            "Реплика: stop\n"
          ]
        }
      ]
    }
  ]
}