## Задача: обучить агента действовать в средах с непрерывным пространством действий, используя алгоритм SAC (Soft Actor Critic) или PPO (Proximal Policy Optimization).

_________________________

### Для задания был исполmзован фреймворк *`google-deepmind-acme`*, который позволил провести тесты и обучение на CPU в среде `pybullet-gym` - `Walker2DBulletEnv`

- В данном [ноутбуке](https://github.com/Mike030668/MIPT_magistratura/blob/main/RL/DZ_3/DZ_3_pybullet_tests.ipynb) были проведены тесты соеды и подбор вариантов управления агентом на основе формул
  
<img src="images/Видео_test1.gif" alt="gif"  width="300"/>  <img src="images/Виде_test2.gif" alt="gif"  width="300"/>  <img src="images/Видео_test3.gif" alt="gif"  width="300"/> 

тесты показали, что можно привестив движение объект путем подбора функций и парамметров
_________________________

<img src="images/sceme.png" alt="png"  width="310"/>  <img src="images/SAC.png" alt="png"  width="400"/>   <img src="images/policy.png" alt="png"  width="260"/> 

`DDPG`, являющийся методом актера-критика, состоит из двух моделей: актера и критика, в отличаи от DQN модели, предсказывающей только распределение

ACTOR — это сеть политик, которая принимает состояние в качестве входных данных и выводит точное действие (непрерывное), а не распределение вероятностей по действиям, таким как `DQN`.

1. Алгоритмы обучения вне политики оценивают и улучшают политику, которая отличается от политики, которая используется для выбора действия. Тоесть, **[Целевая политика  !=  Политика поведения]**.

2. Целевая политика `pi(a|s)`: Это политика, которую агент пытается изучить, т.е. агент изучает функцию значения для этой политики.

3. Политика поведения `b(a|s)`: Это политика, которая используется агентом для выбора действия, т.е. агент следует этой политике для взаимодействия с окружающей средой.

## проведено обучение агента DDPG в [ноутбуке](https://github.com/Mike030668/MIPT_magistratura/blob/main/RL/DZ_3/DZ_3_pybullet_DDPG.ipynb)

<img src="images/Видео_DDPG.gif" alt="gif"  width="550"/>  <img src="images/DDPG.png" alt="png"  width="420"/> 

## проведено обучение агента D4PG в [ноутбуке](https://github.com/Mike030668/MIPT_magistratura/blob/main/RL/DZ_3/DZ_3_pybullet_D4PG.ipynb)

Данный агент имеет возможность в асинхронном исполнении запускатьсь в копиях сред для лучшего обучения и обощения взаимодействия со редой 

<img src="images/Видео_D4PG.gif" alt="gif"  width="550"/>  <img src="images/D4PG.png" alt="png"  width="420"/> 


## Так же проведено обучение агента DMPO в [ноутбуке](https://github.com/Mike030668/MIPT_magistratura/blob/main/RL/DZ_3/DZ_3_pybullet_DMPO.ipynb)
  
**Distributional Maximum a posteriori Policy Optimization (DMPO)**

Максимальная апостериорная оптимизация политики:
MPO — это KL-регуляризованный алгоритм обучения с подкреплением для задач непрерывного управления. Цель включает поочередное максимизацию Q-функции и оптимизацию политики p.
MPO работает в рамках концепции максимизации ожиданий. Не отностится к семейству SAC . Взят для сравнения

<img src="images/Видео_DMPO.gif" alt="gif"  width="400"/> 

_______________________________________________________

Выводы:
Модель D4PG  наиболее устойчива в обучении и дрстаточно набирает более высокие вознаграждения за эпизод.
Хотя первые 80 тыс. шагов loss и reward неустойчивы. 
Модель MPO наоборот учится значительно медленее и плохо справляется с задачей

